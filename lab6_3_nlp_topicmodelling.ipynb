{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tracykimani/Builld-a-quiz-app/blob/main/lab6_3_nlp_topicmodelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 6.3 Topic Modelling\n",
        "## Problem Descriptions\n",
        "\n",
        "Topic modelling aims to discover the hidden semantic structures of a large text corpus, with numerous applications such as automatic categorisation of documents, text mining, text information retrieval, to name a few.\n",
        "\n",
        "The latent Dirichlet allocation (LDA) is a common method for topic modelling, based on the assumption that each document in a corpus is composed by one or more hidden topics, and each topic is supported by a number of words. The process is to find these hidden topics and their supporting words by maximising the posterior probability of the whole corpus given the topics and words.\n",
        "\n",
        "## Implementation and Results"
      ],
      "metadata": {
        "id": "4mA_X1IZKdiL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKmi1fYkXRWj"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from gensim import models, corpora\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FExllgCTXWhI"
      },
      "source": [
        "documents = [\n",
        "  \"\"\"\n",
        "  Though Star Wars initially opened in only 42 theatres, \n",
        "  the film earned almost $3 million in its first week and had grossed \n",
        "  $100 million by the end of the summer. The film won six Academy Awards \n",
        "  along with a special-achievement award for accomplishments in sound, \n",
        "  and it revolutionized the motion picture industry with its advancements \n",
        "  in special effects. Lucas’s effects company, Industrial Light and Magic (ILM), \n",
        "  designed a slew of imaginative alien creatures and mechanical “droids” that \n",
        "  populated a variety of exotic locales. Perhaps most impressive, however, \n",
        "  were the elaborate space battles accomplished with scaled miniatures. \n",
        "  The series continued to make remarkable advancements in the field of \n",
        "  special effects into the 21st century, and ILM became one of the most \n",
        "  successful effects studios utilized by Hollywood. Lucas followed the first \n",
        "  Star Wars film with two sequels, Star Wars: Episode V—The Empire Strikes \n",
        "  Back (1980) and Star Wars: Episode VI—Return of the Jedi (1983). The \n",
        "  franchise thrived in the 1980s and ’90s through the release of videos, a \n",
        "  substantial merchandise line, and the theatrical re-release of \n",
        "  the trilogy in 1997.\n",
        "  \"\"\",\n",
        "  \"\"\"\n",
        "  Lego’s own MI6, its top-secret R & D lab, is on the second floor of a \n",
        "  drab brick structure called the Tech Building. Inside, gearheads in jeans \n",
        "  and fleece pullovers are surrounded by enough electronic ganglia to \n",
        "  jump-start Frankenstein’s monster. Amid a spaghetti of wires and a blaze of \n",
        "  red, green, blue, yellow and purple blocks is an amazing array of robot \n",
        "  prototypes, all capable of exasperating behavior. Some of these marvels \n",
        "  propel themselves on Lego wheels; others skitter around on Lego legs. \n",
        "  There’s a scorpionlike robot that turns sharply, snaps its claws and \n",
        "  searches for an infrared beacon “bug.” There’s a Mohawked android that \n",
        "  flings little red balls as it rumbles. And there’s a fanged robot snake that, \n",
        "  with the wave of a smartphone, shakes, rattles and rolls. Dangle your cell \n",
        "  in front of the serpent’s head and it lunges to bite you.\n",
        "  With bricks, action and hues as vibrant as tropical sunsets, \n",
        "  Lego created a way for novices to learn the basics of structural engineering: \n",
        "  bracing, tension and compression, loading constraints, building to scale. \n",
        "  By combining Lego bricks to sensors, servo motors and microprocessors, those \n",
        "  novices can now explore everything from basic pulleys and belts to computer \n",
        "  programming. “Mindstorms EV3 makes tinkering with machines cool again,” says \n",
        "  Ralph Hempel, author of Lego Spybiotics Secret Agent Training Manual. \n",
        "  \"\"\", \n",
        "  \"\"\"\n",
        "  It’s no exaggeration to say that George Orwell is one of the most \n",
        "  influential writers the UK has ever produced. His novels, including \n",
        "  Nineteen Eighty-Four and Animal Farm, continue to be relevant today, \n",
        "  and his non-fiction writing on truth, politics and society remain \n",
        "  as sharply observed as ever. Born Eric Blair in 1903 in India, \n",
        "  Orwell’s father served the British Empire. While Orwell’s first \n",
        "  job was as a policeman in Burma, he is well-known for being anti-imperialist.\n",
        "  By the time he dies in 1950, Orwell was a successful and respected novelist\n",
        "  and journalist. If you don’t know where to start with his books, our\n",
        "  handy reading guide will help.\n",
        "  There was a moment in 1944 when George Orwell almost lost the original manuscript \n",
        "  for Animal Farm, meaning we could now be without one of the greatest \n",
        "  allegorical novels ever written in English. The completed manuscript for the \n",
        "  book was at Orwell’s home in Kilburn when a V1 flying bomb destroyed the house.\n",
        "  Orwell’s adopted son Richard Blair told the Ham & High newspaper that \n",
        "  the author “spent hours and hours rifling through (bomb site) rubbish)” and \n",
        "  finally found the manuscript. Animal Farm was written between 1943 and 1944, \n",
        "  after Orwell’s experiences during the Spanish Civil War, and was a \n",
        "  condemnation of Stalin’s rule. In modern times, Animal Farm has remained \n",
        "  popular for its insight into authoritarian control or governments that buy \n",
        "  into their power too much. And its popularity has seen it published in a \n",
        "  number of new editions, including this graphic novel, illustrated by Odyr.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chIHYOhwYhAg",
        "outputId": "2150458a-01ea-4493-95f2-84eccb64a6b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Clean the data by using stemming and stopwords removal\n",
        "nltk.download('stopwords')\n",
        "stemmer = SnowballStemmer('english')\n",
        "stop_words = stopwords.words('english')\n",
        "texts = [\n",
        "  [stemmer.stem(word) for word in document.lower().split() if word not in stop_words]\n",
        "  for document in documents\n",
        "  ]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFmXf5OWK5j9",
        "outputId": "90ee800c-267a-430b-af1f-0b688fc4304e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['though',\n",
              "  'star',\n",
              "  'war',\n",
              "  'initi',\n",
              "  'open',\n",
              "  '42',\n",
              "  'theatres,',\n",
              "  'film',\n",
              "  'earn',\n",
              "  'almost',\n",
              "  '$3',\n",
              "  'million',\n",
              "  'first',\n",
              "  'week',\n",
              "  'gross',\n",
              "  '$100',\n",
              "  'million',\n",
              "  'end',\n",
              "  'summer.',\n",
              "  'film',\n",
              "  'six',\n",
              "  'academi',\n",
              "  'award',\n",
              "  'along',\n",
              "  'special-achiev',\n",
              "  'award',\n",
              "  'accomplish',\n",
              "  'sound,',\n",
              "  'revolution',\n",
              "  'motion',\n",
              "  'pictur',\n",
              "  'industri',\n",
              "  'advanc',\n",
              "  'special',\n",
              "  'effects.',\n",
              "  'luca',\n",
              "  'effect',\n",
              "  'company,',\n",
              "  'industri',\n",
              "  'light',\n",
              "  'magic',\n",
              "  '(ilm),',\n",
              "  'design',\n",
              "  'slew',\n",
              "  'imagin',\n",
              "  'alien',\n",
              "  'creatur',\n",
              "  'mechan',\n",
              "  '“droids”',\n",
              "  'popul',\n",
              "  'varieti',\n",
              "  'exot',\n",
              "  'locales.',\n",
              "  'perhap',\n",
              "  'impressive,',\n",
              "  'however,',\n",
              "  'elabor',\n",
              "  'space',\n",
              "  'battl',\n",
              "  'accomplish',\n",
              "  'scale',\n",
              "  'miniatures.',\n",
              "  'seri',\n",
              "  'continu',\n",
              "  'make',\n",
              "  'remark',\n",
              "  'advanc',\n",
              "  'field',\n",
              "  'special',\n",
              "  'effect',\n",
              "  '21st',\n",
              "  'century,',\n",
              "  'ilm',\n",
              "  'becam',\n",
              "  'one',\n",
              "  'success',\n",
              "  'effect',\n",
              "  'studio',\n",
              "  'util',\n",
              "  'hollywood.',\n",
              "  'luca',\n",
              "  'follow',\n",
              "  'first',\n",
              "  'star',\n",
              "  'war',\n",
              "  'film',\n",
              "  'two',\n",
              "  'sequels,',\n",
              "  'star',\n",
              "  'wars:',\n",
              "  'episod',\n",
              "  'v—the',\n",
              "  'empir',\n",
              "  'strike',\n",
              "  'back',\n",
              "  '(1980)',\n",
              "  'star',\n",
              "  'wars:',\n",
              "  'episod',\n",
              "  'vi—return',\n",
              "  'jedi',\n",
              "  '(1983).',\n",
              "  'franchis',\n",
              "  'thrive',\n",
              "  '1980s',\n",
              "  '90s',\n",
              "  'releas',\n",
              "  'videos,',\n",
              "  'substanti',\n",
              "  'merchandis',\n",
              "  'line,',\n",
              "  'theatric',\n",
              "  're-releas',\n",
              "  'trilog',\n",
              "  '1997.'],\n",
              " ['lego',\n",
              "  'mi6,',\n",
              "  'top-secret',\n",
              "  'r',\n",
              "  '&',\n",
              "  'lab,',\n",
              "  'second',\n",
              "  'floor',\n",
              "  'drab',\n",
              "  'brick',\n",
              "  'structur',\n",
              "  'call',\n",
              "  'tech',\n",
              "  'building.',\n",
              "  'inside,',\n",
              "  'gearhead',\n",
              "  'jean',\n",
              "  'fleec',\n",
              "  'pullov',\n",
              "  'surround',\n",
              "  'enough',\n",
              "  'electron',\n",
              "  'ganglia',\n",
              "  'jump-start',\n",
              "  'frankenstein',\n",
              "  'monster.',\n",
              "  'amid',\n",
              "  'spaghetti',\n",
              "  'wire',\n",
              "  'blaze',\n",
              "  'red,',\n",
              "  'green,',\n",
              "  'blue,',\n",
              "  'yellow',\n",
              "  'purpl',\n",
              "  'block',\n",
              "  'amaz',\n",
              "  'array',\n",
              "  'robot',\n",
              "  'prototypes,',\n",
              "  'capabl',\n",
              "  'exasper',\n",
              "  'behavior.',\n",
              "  'marvel',\n",
              "  'propel',\n",
              "  'lego',\n",
              "  'wheels;',\n",
              "  'other',\n",
              "  'skitter',\n",
              "  'around',\n",
              "  'lego',\n",
              "  'legs.',\n",
              "  'there',\n",
              "  'scorpionlik',\n",
              "  'robot',\n",
              "  'turn',\n",
              "  'sharply,',\n",
              "  'snap',\n",
              "  'claw',\n",
              "  'search',\n",
              "  'infrar',\n",
              "  'beacon',\n",
              "  '“bug.”',\n",
              "  'there',\n",
              "  'mohawk',\n",
              "  'android',\n",
              "  'fling',\n",
              "  'littl',\n",
              "  'red',\n",
              "  'ball',\n",
              "  'rumbles.',\n",
              "  'there',\n",
              "  'fang',\n",
              "  'robot',\n",
              "  'snake',\n",
              "  'that,',\n",
              "  'wave',\n",
              "  'smartphone,',\n",
              "  'shakes,',\n",
              "  'rattl',\n",
              "  'rolls.',\n",
              "  'dangl',\n",
              "  'cell',\n",
              "  'front',\n",
              "  'serpent',\n",
              "  'head',\n",
              "  'lung',\n",
              "  'bite',\n",
              "  'you.',\n",
              "  'bricks,',\n",
              "  'action',\n",
              "  'hue',\n",
              "  'vibrant',\n",
              "  'tropic',\n",
              "  'sunsets,',\n",
              "  'lego',\n",
              "  'creat',\n",
              "  'way',\n",
              "  'novic',\n",
              "  'learn',\n",
              "  'basic',\n",
              "  'structur',\n",
              "  'engineering:',\n",
              "  'bracing,',\n",
              "  'tension',\n",
              "  'compression,',\n",
              "  'load',\n",
              "  'constraints,',\n",
              "  'build',\n",
              "  'scale.',\n",
              "  'combin',\n",
              "  'lego',\n",
              "  'brick',\n",
              "  'sensors,',\n",
              "  'servo',\n",
              "  'motor',\n",
              "  'microprocessors,',\n",
              "  'novic',\n",
              "  'explor',\n",
              "  'everyth',\n",
              "  'basic',\n",
              "  'pulley',\n",
              "  'belt',\n",
              "  'comput',\n",
              "  'programming.',\n",
              "  '“mindstorm',\n",
              "  'ev3',\n",
              "  'make',\n",
              "  'tinker',\n",
              "  'machin',\n",
              "  'cool',\n",
              "  'again,”',\n",
              "  'say',\n",
              "  'ralph',\n",
              "  'hempel,',\n",
              "  'author',\n",
              "  'lego',\n",
              "  'spybiot',\n",
              "  'secret',\n",
              "  'agent',\n",
              "  'train',\n",
              "  'manual.'],\n",
              " ['it',\n",
              "  'exagger',\n",
              "  'say',\n",
              "  'georg',\n",
              "  'orwel',\n",
              "  'one',\n",
              "  'influenti',\n",
              "  'writer',\n",
              "  'uk',\n",
              "  'ever',\n",
              "  'produced.',\n",
              "  'novels,',\n",
              "  'includ',\n",
              "  'nineteen',\n",
              "  'eighty-four',\n",
              "  'anim',\n",
              "  'farm,',\n",
              "  'continu',\n",
              "  'relev',\n",
              "  'today,',\n",
              "  'non-fict',\n",
              "  'write',\n",
              "  'truth,',\n",
              "  'polit',\n",
              "  'societi',\n",
              "  'remain',\n",
              "  'sharpli',\n",
              "  'observ',\n",
              "  'ever.',\n",
              "  'born',\n",
              "  'eric',\n",
              "  'blair',\n",
              "  '1903',\n",
              "  'india,',\n",
              "  'orwel',\n",
              "  'father',\n",
              "  'serv',\n",
              "  'british',\n",
              "  'empire.',\n",
              "  'orwel',\n",
              "  'first',\n",
              "  'job',\n",
              "  'policeman',\n",
              "  'burma,',\n",
              "  'well-known',\n",
              "  'anti-imperialist.',\n",
              "  'time',\n",
              "  'die',\n",
              "  '1950,',\n",
              "  'orwel',\n",
              "  'success',\n",
              "  'respect',\n",
              "  'novelist',\n",
              "  'journalist.',\n",
              "  \"don't\",\n",
              "  'know',\n",
              "  'start',\n",
              "  'books,',\n",
              "  'handi',\n",
              "  'read',\n",
              "  'guid',\n",
              "  'help.',\n",
              "  'moment',\n",
              "  '1944',\n",
              "  'georg',\n",
              "  'orwel',\n",
              "  'almost',\n",
              "  'lost',\n",
              "  'origin',\n",
              "  'manuscript',\n",
              "  'anim',\n",
              "  'farm,',\n",
              "  'mean',\n",
              "  'could',\n",
              "  'without',\n",
              "  'one',\n",
              "  'greatest',\n",
              "  'allegor',\n",
              "  'novel',\n",
              "  'ever',\n",
              "  'written',\n",
              "  'english.',\n",
              "  'complet',\n",
              "  'manuscript',\n",
              "  'book',\n",
              "  'orwel',\n",
              "  'home',\n",
              "  'kilburn',\n",
              "  'v1',\n",
              "  'fli',\n",
              "  'bomb',\n",
              "  'destroy',\n",
              "  'house.',\n",
              "  'orwel',\n",
              "  'adopt',\n",
              "  'son',\n",
              "  'richard',\n",
              "  'blair',\n",
              "  'told',\n",
              "  'ham',\n",
              "  '&',\n",
              "  'high',\n",
              "  'newspap',\n",
              "  'author',\n",
              "  '“spent',\n",
              "  'hour',\n",
              "  'hour',\n",
              "  'rifl',\n",
              "  '(bomb',\n",
              "  'site)',\n",
              "  'rubbish)”',\n",
              "  'final',\n",
              "  'found',\n",
              "  'manuscript.',\n",
              "  'anim',\n",
              "  'farm',\n",
              "  'written',\n",
              "  '1943',\n",
              "  '1944,',\n",
              "  'orwel',\n",
              "  'experi',\n",
              "  'spanish',\n",
              "  'civil',\n",
              "  'war,',\n",
              "  'condemn',\n",
              "  'stalin',\n",
              "  'rule.',\n",
              "  'modern',\n",
              "  'times,',\n",
              "  'anim',\n",
              "  'farm',\n",
              "  'remain',\n",
              "  'popular',\n",
              "  'insight',\n",
              "  'authoritarian',\n",
              "  'control',\n",
              "  'govern',\n",
              "  'buy',\n",
              "  'power',\n",
              "  'much.',\n",
              "  'popular',\n",
              "  'seen',\n",
              "  'publish',\n",
              "  'number',\n",
              "  'new',\n",
              "  'editions,',\n",
              "  'includ',\n",
              "  'graphic',\n",
              "  'novel,',\n",
              "  'illustr',\n",
              "  'odyr.']]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xljrygKZVbT"
      },
      "source": [
        "# Create a dictionary from the words\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "\n",
        "# Create a document-term matrix\n",
        "doc_term_mat = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Generate the LDA model \n",
        "num_topics = 3\n",
        "ldamodel = models.ldamodel.LdaModel(doc_term_mat, \n",
        "        num_topics=num_topics, id2word=dictionary, passes=25)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_term_mat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_o2NQcOwONQC",
        "outputId": "5659a380-b88b-4ed6-d89e-590829bc39b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 1),\n",
              "  (1, 1),\n",
              "  (2, 1),\n",
              "  (3, 1),\n",
              "  (4, 1),\n",
              "  (5, 1),\n",
              "  (6, 1),\n",
              "  (7, 1),\n",
              "  (8, 1),\n",
              "  (9, 1),\n",
              "  (10, 1),\n",
              "  (11, 2),\n",
              "  (12, 2),\n",
              "  (13, 1),\n",
              "  (14, 1),\n",
              "  (15, 1),\n",
              "  (16, 2),\n",
              "  (17, 1),\n",
              "  (18, 1),\n",
              "  (19, 1),\n",
              "  (20, 1),\n",
              "  (21, 1),\n",
              "  (22, 1),\n",
              "  (23, 1),\n",
              "  (24, 1),\n",
              "  (25, 1),\n",
              "  (26, 3),\n",
              "  (27, 1),\n",
              "  (28, 1),\n",
              "  (29, 1),\n",
              "  (30, 1),\n",
              "  (31, 2),\n",
              "  (32, 1),\n",
              "  (33, 1),\n",
              "  (34, 3),\n",
              "  (35, 2),\n",
              "  (36, 1),\n",
              "  (37, 1),\n",
              "  (38, 1),\n",
              "  (39, 1),\n",
              "  (40, 1),\n",
              "  (41, 1),\n",
              "  (42, 1),\n",
              "  (43, 1),\n",
              "  (44, 2),\n",
              "  (45, 1),\n",
              "  (46, 1),\n",
              "  (47, 1),\n",
              "  (48, 1),\n",
              "  (49, 1),\n",
              "  (50, 2),\n",
              "  (51, 1),\n",
              "  (52, 1),\n",
              "  (53, 1),\n",
              "  (54, 1),\n",
              "  (55, 2),\n",
              "  (56, 1),\n",
              "  (57, 1),\n",
              "  (58, 1),\n",
              "  (59, 1),\n",
              "  (60, 1),\n",
              "  (61, 1),\n",
              "  (62, 1),\n",
              "  (63, 1),\n",
              "  (64, 1),\n",
              "  (65, 1),\n",
              "  (66, 1),\n",
              "  (67, 1),\n",
              "  (68, 1),\n",
              "  (69, 1),\n",
              "  (70, 1),\n",
              "  (71, 1),\n",
              "  (72, 1),\n",
              "  (73, 1),\n",
              "  (74, 2),\n",
              "  (75, 1),\n",
              "  (76, 4),\n",
              "  (77, 1),\n",
              "  (78, 1),\n",
              "  (79, 1),\n",
              "  (80, 1),\n",
              "  (81, 1),\n",
              "  (82, 1),\n",
              "  (83, 1),\n",
              "  (84, 1),\n",
              "  (85, 1),\n",
              "  (86, 1),\n",
              "  (87, 1),\n",
              "  (88, 1),\n",
              "  (89, 1),\n",
              "  (90, 1),\n",
              "  (91, 1),\n",
              "  (92, 1),\n",
              "  (93, 2),\n",
              "  (94, 2),\n",
              "  (95, 1),\n",
              "  (96, 1)],\n",
              " [(52, 1),\n",
              "  (97, 1),\n",
              "  (98, 1),\n",
              "  (99, 1),\n",
              "  (100, 1),\n",
              "  (101, 1),\n",
              "  (102, 1),\n",
              "  (103, 1),\n",
              "  (104, 1),\n",
              "  (105, 1),\n",
              "  (106, 1),\n",
              "  (107, 1),\n",
              "  (108, 2),\n",
              "  (109, 1),\n",
              "  (110, 1),\n",
              "  (111, 1),\n",
              "  (112, 1),\n",
              "  (113, 1),\n",
              "  (114, 1),\n",
              "  (115, 1),\n",
              "  (116, 1),\n",
              "  (117, 2),\n",
              "  (118, 1),\n",
              "  (119, 1),\n",
              "  (120, 1),\n",
              "  (121, 1),\n",
              "  (122, 1),\n",
              "  (123, 1),\n",
              "  (124, 1),\n",
              "  (125, 1),\n",
              "  (126, 1),\n",
              "  (127, 1),\n",
              "  (128, 1),\n",
              "  (129, 1),\n",
              "  (130, 1),\n",
              "  (131, 1),\n",
              "  (132, 1),\n",
              "  (133, 1),\n",
              "  (134, 1),\n",
              "  (135, 1),\n",
              "  (136, 1),\n",
              "  (137, 1),\n",
              "  (138, 1),\n",
              "  (139, 1),\n",
              "  (140, 1),\n",
              "  (141, 1),\n",
              "  (142, 1),\n",
              "  (143, 1),\n",
              "  (144, 1),\n",
              "  (145, 1),\n",
              "  (146, 1),\n",
              "  (147, 1),\n",
              "  (148, 1),\n",
              "  (149, 1),\n",
              "  (150, 1),\n",
              "  (151, 1),\n",
              "  (152, 1),\n",
              "  (153, 1),\n",
              "  (154, 1),\n",
              "  (155, 1),\n",
              "  (156, 1),\n",
              "  (157, 1),\n",
              "  (158, 6),\n",
              "  (159, 1),\n",
              "  (160, 1),\n",
              "  (161, 1),\n",
              "  (162, 1),\n",
              "  (163, 1),\n",
              "  (164, 1),\n",
              "  (165, 1),\n",
              "  (166, 1),\n",
              "  (167, 1),\n",
              "  (168, 1),\n",
              "  (169, 1),\n",
              "  (170, 1),\n",
              "  (171, 2),\n",
              "  (172, 1),\n",
              "  (173, 1),\n",
              "  (174, 1),\n",
              "  (175, 1),\n",
              "  (176, 1),\n",
              "  (177, 1),\n",
              "  (178, 1),\n",
              "  (179, 1),\n",
              "  (180, 1),\n",
              "  (181, 1),\n",
              "  (182, 1),\n",
              "  (183, 1),\n",
              "  (184, 3),\n",
              "  (185, 1),\n",
              "  (186, 1),\n",
              "  (187, 1),\n",
              "  (188, 1),\n",
              "  (189, 1),\n",
              "  (190, 1),\n",
              "  (191, 1),\n",
              "  (192, 1),\n",
              "  (193, 1),\n",
              "  (194, 1),\n",
              "  (195, 1),\n",
              "  (196, 1),\n",
              "  (197, 1),\n",
              "  (198, 1),\n",
              "  (199, 1),\n",
              "  (200, 1),\n",
              "  (201, 1),\n",
              "  (202, 1),\n",
              "  (203, 1),\n",
              "  (204, 2),\n",
              "  (205, 1),\n",
              "  (206, 1),\n",
              "  (207, 1),\n",
              "  (208, 1),\n",
              "  (209, 1),\n",
              "  (210, 3),\n",
              "  (211, 1),\n",
              "  (212, 1),\n",
              "  (213, 1),\n",
              "  (214, 1),\n",
              "  (215, 1),\n",
              "  (216, 1),\n",
              "  (217, 1),\n",
              "  (218, 1),\n",
              "  (219, 1),\n",
              "  (220, 1),\n",
              "  (221, 1),\n",
              "  (222, 1),\n",
              "  (223, 1),\n",
              "  (224, 1)],\n",
              " [(14, 1),\n",
              "  (22, 1),\n",
              "  (35, 1),\n",
              "  (58, 2),\n",
              "  (80, 1),\n",
              "  (97, 1),\n",
              "  (106, 1),\n",
              "  (187, 1),\n",
              "  (225, 1),\n",
              "  (226, 1),\n",
              "  (227, 1),\n",
              "  (228, 1),\n",
              "  (229, 1),\n",
              "  (230, 1),\n",
              "  (231, 1),\n",
              "  (232, 1),\n",
              "  (233, 4),\n",
              "  (234, 1),\n",
              "  (235, 1),\n",
              "  (236, 2),\n",
              "  (237, 1),\n",
              "  (238, 1),\n",
              "  (239, 1),\n",
              "  (240, 1),\n",
              "  (241, 1),\n",
              "  (242, 1),\n",
              "  (243, 1),\n",
              "  (244, 1),\n",
              "  (245, 1),\n",
              "  (246, 1),\n",
              "  (247, 1),\n",
              "  (248, 1),\n",
              "  (249, 1),\n",
              "  (250, 1),\n",
              "  (251, 1),\n",
              "  (252, 1),\n",
              "  (253, 1),\n",
              "  (254, 1),\n",
              "  (255, 1),\n",
              "  (256, 1),\n",
              "  (257, 2),\n",
              "  (258, 1),\n",
              "  (259, 1),\n",
              "  (260, 1),\n",
              "  (261, 2),\n",
              "  (262, 2),\n",
              "  (263, 1),\n",
              "  (264, 1),\n",
              "  (265, 1),\n",
              "  (266, 1),\n",
              "  (267, 2),\n",
              "  (268, 1),\n",
              "  (269, 1),\n",
              "  (270, 1),\n",
              "  (271, 1),\n",
              "  (272, 1),\n",
              "  (273, 1),\n",
              "  (274, 1),\n",
              "  (275, 1),\n",
              "  (276, 1),\n",
              "  (277, 2),\n",
              "  (278, 1),\n",
              "  (279, 1),\n",
              "  (280, 2),\n",
              "  (281, 1),\n",
              "  (282, 1),\n",
              "  (283, 1),\n",
              "  (284, 1),\n",
              "  (285, 1),\n",
              "  (286, 1),\n",
              "  (287, 1),\n",
              "  (288, 1),\n",
              "  (289, 1),\n",
              "  (290, 2),\n",
              "  (291, 1),\n",
              "  (292, 1),\n",
              "  (293, 1),\n",
              "  (294, 1),\n",
              "  (295, 1),\n",
              "  (296, 1),\n",
              "  (297, 1),\n",
              "  (298, 1),\n",
              "  (299, 1),\n",
              "  (300, 1),\n",
              "  (301, 1),\n",
              "  (302, 1),\n",
              "  (303, 1),\n",
              "  (304, 1),\n",
              "  (305, 1),\n",
              "  (306, 1),\n",
              "  (307, 1),\n",
              "  (308, 8),\n",
              "  (309, 1),\n",
              "  (310, 1),\n",
              "  (311, 2),\n",
              "  (312, 1),\n",
              "  (313, 1),\n",
              "  (314, 1),\n",
              "  (315, 1),\n",
              "  (316, 1),\n",
              "  (317, 2),\n",
              "  (318, 1),\n",
              "  (319, 1),\n",
              "  (320, 1),\n",
              "  (321, 1),\n",
              "  (322, 1),\n",
              "  (323, 1),\n",
              "  (324, 1),\n",
              "  (325, 1),\n",
              "  (326, 1),\n",
              "  (327, 1),\n",
              "  (328, 1),\n",
              "  (329, 1),\n",
              "  (330, 1),\n",
              "  (331, 1),\n",
              "  (332, 1),\n",
              "  (333, 1),\n",
              "  (334, 1),\n",
              "  (335, 1),\n",
              "  (336, 1),\n",
              "  (337, 1),\n",
              "  (338, 1),\n",
              "  (339, 1),\n",
              "  (340, 1),\n",
              "  (341, 1),\n",
              "  (342, 1),\n",
              "  (343, 1),\n",
              "  (344, 2),\n",
              "  (345, 1)]]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3XG_FXPhMzq",
        "outputId": "84dac667-a931-4558-c09e-96d01b4004dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "num_words = 5\n",
        "for i in range(num_topics):\n",
        "  print(ldamodel.print_topic(i, topn=num_words))\n",
        "\n",
        "print('\\nTop ' + str(num_words) + ' contributing words to each topic:')\n",
        "for item in ldamodel.print_topics(num_topics=num_topics, num_words=num_words):\n",
        "    print('\\nTopic', item[0])\n",
        "    list_of_strings = item[1].split(' + ')\n",
        "    for text in list_of_strings:\n",
        "        details = text.split('*')\n",
        "        print(\"%-12s:%0.2f%%\" %(details[1], 100*float(details[0])))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.022*\"orwel\" + 0.011*\"star\" + 0.011*\"anim\" + 0.009*\"film\" + 0.009*\"first\"\n",
            "0.003*\"orwel\" + 0.003*\"anim\" + 0.003*\"farm\" + 0.003*\"lego\" + 0.003*\"remain\"\n",
            "0.025*\"lego\" + 0.013*\"robot\" + 0.013*\"there\" + 0.009*\"structur\" + 0.009*\"brick\"\n",
            "\n",
            "Top 5 contributing words to each topic:\n",
            "\n",
            "Topic 0\n",
            "\"orwel\"     :2.20%\n",
            "\"star\"      :1.10%\n",
            "\"anim\"      :1.10%\n",
            "\"film\"      :0.90%\n",
            "\"first\"     :0.90%\n",
            "\n",
            "Topic 1\n",
            "\"orwel\"     :0.30%\n",
            "\"anim\"      :0.30%\n",
            "\"farm\"      :0.30%\n",
            "\"lego\"      :0.30%\n",
            "\"remain\"    :0.30%\n",
            "\n",
            "Topic 2\n",
            "\"lego\"      :2.50%\n",
            "\"robot\"     :1.30%\n",
            "\"there\"     :1.30%\n",
            "\"structur\"  :0.90%\n",
            "\"brick\"     :0.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZmJOtjhQ6WX",
        "outputId": "678529d2-1418-4f33-9491-499097bd18f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "new_docs = [\n",
        "  \"\"\"\n",
        "  We’ve been working with LEGO Star Wars since the beginning, \n",
        "  Chris and I, and we acknowledged right away that there was a huge \n",
        "  [adult audience]. We just saw it in the office when we started with \n",
        "  LEGO Star Wars — how many of my colleagues were super interested and \n",
        "  into that. It was already, in the year 2000, the second year of LEGO Star \n",
        "  Wars, that we introduced the Ultimate Collector Series models. At that time, \n",
        "  I think they were marked 16+, but we knew that these were something that \n",
        "  were definitely more complex and targeted to an older audience. And because \n",
        "  of that we had a special packaging. Do you remember, Chris? \n",
        "  It was black and white.\n",
        "  \"\"\"\n",
        "]\n",
        "\n",
        "new_texts = [\n",
        "  [stemmer.stem(word) for word in document.lower().split() if word not in stop_words]\n",
        "  for document in new_docs\n",
        "  ]\n",
        "new_doc_term_mat = [dictionary.doc2bow(text) for text in new_texts]\n",
        "\n",
        "vector = ldamodel[new_doc_term_mat]\n",
        "print(vector[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 0.6364652), (1, 0.026084848), (2, 0.33744997)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_docs = [\n",
        "  \"\"\"\n",
        "  In the decades since the publication of George Orwell’s seminal work of \n",
        "  anti-Stalinist satire, we have seen the collapse of the regime that \n",
        "  disturbed and inspired its author; the beginning and end of the \n",
        "  Cold War, with all its attendant horrors; and the rise and fall of \n",
        "  any number of would-be Napoleons, both at home and abroad. Animal Farm, \n",
        "  once a work so controversial that it seemed unlikely to find a publisher, \n",
        "  has served for so long, and in so many school curriculums, as the \n",
        "  predominant introduction to the concept of totalitarianism that it is \n",
        "  in danger of being perceived as trite.\n",
        "  \"\"\"\n",
        "]\n",
        "\n",
        "new_texts = [\n",
        "  [stemmer.stem(word) for word in document.lower().split() if word not in stop_words]\n",
        "  for document in new_docs\n",
        "  ]\n",
        "new_doc_term_mat = [dictionary.doc2bow(text) for text in new_texts]\n",
        "\n",
        "vector = ldamodel[new_doc_term_mat]\n",
        "print(vector[0])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWAdtJwT-5M8",
        "outputId": "9fd43db5-34f3-4526-d1aa-019f78e800f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 0.9386559), (1, 0.030818595), (2, 0.03052543)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussions\n",
        "\n",
        "In this task, I perform topic modelling using an LDA model.\n",
        "The three documents I used in the corpus, were gotten from different online articles involving the topics:  \"Star Wars\", \"LEGO\", and \"George Orwell's Animal Farm\".\n",
        "\n",
        "I then cleaned the data by removing stop words and performming stemming with a \"SnowballStemmer\" provided by the ```nltk``` package.\n",
        "\n",
        "Then a dictionary was created from the cleaned words, and a document-term matrix was built using the bag-of-words model.\n",
        "\n",
        "I then applied the LDA model to find the topics and their supporting words (where I opted to print the top 5 supporting words per topic).\n",
        "\n",
        "After that, the trained model was applied to new text; to find out if it could determine whether the topic(s) in the new text were related to the topics of the documents that were in the corpus, and to what degree this new text corresponds to each topic in the corpus.\n",
        "\n",
        "\n",
        "\n",
        "## Results\n",
        "\n",
        "I applied the model to two different text examples. For the first one, it is about \"LEGO Star Wars\", and is a transcription of an interview I found online. The model judged the text to be 61% related to the topic, \"Star Wars\"; 35% related to the topic, \"LEGO\"; and 4% related to the topic, \"George Orwell's Animal Farm\".\n",
        "\n",
        "For the second example, I took the exerpt of text from an article about the 75th Anniversary of Orwell's Animal Farm.\n",
        "\n",
        "The model judged that this text was about 8% related to the \"Star Wars\" topic; about 3% related to the topic, \"LEGO\"; and a significant 89% related to the topic, \"George Orwell's Animal Farm\".\n",
        "\n",
        "We can see that the model performs well in all instances and is able to identify what topics are contained within these texts.\n"
      ],
      "metadata": {
        "id": "Zm5Z-WBNKbUz"
      }
    }
  ]
}